{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat scenario 1 results from paper Multiple Network Embedding for Anomaly Detection in Time Series of Graphs(link: file:///C:/Users/16827/OneDrive/Documents/Fall_2020_semester_jhu/NDD/Class_material/OMNIvsMASE.pdf)\n",
    "\n",
    "#import all needed packages\n",
    "import graspy\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from graspy.simulations import rdpg\n",
    "from numpy import linalg as LA\n",
    "from graspy.inference import LatentPositionTest\n",
    "from graspy.plot import heatmap, pairplot\n",
    "from graspy.embed import OmnibusEmbed, select_dimension, AdjacencySpectralEmbed, MultipleASE\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "from graspy.utils import import_graph, is_symmetric\n",
    "from graspy.inference import base\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation and contributors.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class BaseInference(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Base class for inference tasks such as semiparametric latent position test\n",
    "    and nonparametric latent distribution test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : None (default), or int\n",
    "        Number of embedding dimensions. If None, the optimal embedding\n",
    "        dimensions are chosen via the Zhu and Godsi algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=None):\n",
    "        if (not isinstance(n_components, (int, np.integer))) and (\n",
    "            n_components is not None\n",
    "        ):\n",
    "            raise TypeError(\"n_components must be int or np.integer\")\n",
    "        if n_components is not None and n_components <= 0:\n",
    "            raise ValueError(\n",
    "                \"Cannot embed into {} dimensions, must be greater than 0\".format(\n",
    "                    n_components\n",
    "                )\n",
    "            )\n",
    "        self.n_components = n_components\n",
    "\n",
    "    @abstractmethod\n",
    "    def _embed(self, A1, A2, n_componets):\n",
    "        \"\"\"\n",
    "        Computes the latent positions of input graphs\n",
    "        Parameters\n",
    "        ----------\n",
    "        A1 : np.ndarray, shape (n_vertices, n_vertices)\n",
    "            Adjacency matrix of the first graph\n",
    "        A2 : np.ndarray, shape (n_vertices, n_vertices)\n",
    "            Adjacency matrix of the second graph\n",
    "        Returns\n",
    "        -------\n",
    "        X1_hat : array-like, shape (n_vertices, n_components)\n",
    "            Estimated latent positions of the vertices in the first graph\n",
    "        X2_hat : array-like, shape(n_vertices, n_components)\n",
    "            Estimated latent positions of the vertices in the second graph\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, A1, A2):\n",
    "        \"\"\"\n",
    "        Compute the test statistic and the null distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        A1, A2 : nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray\n",
    "            The two graphs to run a hypothesis test on.\n",
    "            If np.ndarray, shape must be ``(n_vertices, n_vertices)`` for both\n",
    "            graphs, where ``n_vertices`` is the same for both\n",
    "        Returns\n",
    "        ------\n",
    "        self\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self, A1, A2):\n",
    "        \"\"\"\n",
    "        Fits the model and returns the p-value\n",
    "        Parameters\n",
    "        ----------\n",
    "        A1, A2 : nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray\n",
    "            The two graphs to run a hypothesis test on.\n",
    "            If np.ndarray, shape must be ``(n_vertices, n_vertices)`` for both\n",
    "            graphs, where ``n_vertices`` is the same for both\n",
    "        Returns\n",
    "        ------\n",
    "        p_value_ : float\n",
    "            The overall p value from the test\n",
    "        \"\"\"\n",
    "        self.fit(A1, A2)\n",
    "        return self.p_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create functions to compute the test statistics for GraphAD, VertexAD\n",
    "def t_statistics_g(X, Y):\n",
    "    '''This function accepts as input the latent position matrix of the two graphs for which the test statistic t is being computed\n",
    "       According to the formula t = ||X - Y||(l2norm) where X indicates currrent graph and Y the previous graph in the time series\n",
    "       and returns the computed t statistic'''\n",
    "    y = LA.norm(X-Y, ord = 2)\n",
    "    return y\n",
    "\n",
    "def t_statistics_v(X, Y):\n",
    "    '''This function accepts as input the latent position matrix of the two graphs for which the test statistic t is being computed.\n",
    "       Then computes t-statistic for each vertex using latent position X(i) for the vertex i of graph X and Y(i) for the vertex i of graph Y.\n",
    "       Graph X is latent position of current time series graph G(t) and Y is previous time G(t-1).\n",
    "       The statistic is computed according to the formula:\n",
    "       t(i) = ||X(i) - Y(i)||(l2-norm).\n",
    "       Returns a tuple consisting of each individual vertex test statistic.\n",
    "       Matrices X and Y must have the same dimensions'''\n",
    "    y = ()\n",
    "    for i in range(len(X)):\n",
    "        y = y + (LA.norm(X[i]-Y[i], ord = 2),)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentPositionTest_X(BaseInference):\n",
    "    def __init__(\n",
    "        self, embedding=\"ase\", n_components=None, n_bootstraps=500, test_case=\"rotation\"\n",
    "    ):\n",
    "        if type(embedding) is not str:\n",
    "            raise TypeError(\"embedding must be str\")\n",
    "        if type(n_bootstraps) is not int:\n",
    "            raise TypeError()\n",
    "        if type(test_case) is not str:\n",
    "            raise TypeError()\n",
    "        if n_bootstraps < 1:\n",
    "            raise ValueError(\n",
    "                \"{} is invalid number of bootstraps, must be greater than 1\".format(\n",
    "                    n_bootstraps\n",
    "                )\n",
    "            )\n",
    "        if embedding not in [\"ase\", \"omnibus\"]:\n",
    "            raise ValueError(\"{} is not a valid embedding method.\".format(embedding))\n",
    "        if test_case not in [\"rotation\", \"scalar-rotation\", \"diagonal-rotation\"]:\n",
    "            raise ValueError(\n",
    "                \"test_case must be one of 'rotation', 'scalar-rotation',\"\n",
    "                + \"'diagonal-rotation'\"\n",
    "            )\n",
    "\n",
    "        super().__init__(n_components=n_components)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.n_bootstraps = n_bootstraps\n",
    "        self.test_case = test_case\n",
    "        # paper uses these always, but could be kwargs eventually. need to test\n",
    "        self.rescale = False\n",
    "        self.loops = False\n",
    "\n",
    "    def _bootstrap(self, X_hat):\n",
    "        t_bootstrap = np.zeros(self.n_bootstraps)\n",
    "        for i in range(self.n_bootstraps):\n",
    "            A1_simulated = rdpg(X_hat, rescale=self.rescale, loops=self.loops)\n",
    "            A2_simulated = rdpg(X_hat, rescale=self.rescale, loops=self.loops)\n",
    "            X1_hat_simulated, X2_hat_simulated = self._embed(\n",
    "                A1_simulated, A2_simulated, check_lcc=False\n",
    "            )\n",
    "            t_bootstrap[i] = self._difference_norm(X1_hat_simulated, X2_hat_simulated)\n",
    "        return t_bootstrap\n",
    "    \n",
    "    def _embed(self, A1, A2, check_lcc=True):\n",
    "        if self.embedding == \"ase\":\n",
    "            X1_hat = AdjacencySpectralEmbed(\n",
    "                n_components=self.n_components, check_lcc=check_lcc\n",
    "            ).fit_transform(A1)\n",
    "            X2_hat = AdjacencySpectralEmbed(\n",
    "                n_components=self.n_components, check_lcc=check_lcc\n",
    "            ).fit_transform(A2)\n",
    "        elif self.embedding == \"omnibus\":\n",
    "            X_hat_compound = OmnibusEmbed(\n",
    "                n_components=self.n_components, check_lcc=check_lcc\n",
    "            ).fit_transform((A1, A2))\n",
    "            X1_hat = X_hat_compound[0]\n",
    "            X2_hat = X_hat_compound[1]\n",
    "        return (X1_hat, X2_hat)\n",
    "\n",
    "\n",
    "    def _difference_norm(self, X1, X2):\n",
    "        if self.embedding in [\"ase\"]:\n",
    "            if self.test_case == \"rotation\":\n",
    "                R = orthogonal_procrustes(X1, X2)[0]\n",
    "                return np.linalg.norm(X1 @ R - X2)\n",
    "            elif self.test_case == \"scalar-rotation\":\n",
    "                R, s = orthogonal_procrustes(X1, X2)\n",
    "                return np.linalg.norm(s / np.sum(X1 ** 2) * X1 @ R - X2)\n",
    "            elif self.test_case == \"diagonal-rotation\":\n",
    "                normX1 = np.sum(X1 ** 2, axis=1)\n",
    "                normX2 = np.sum(X2 ** 2, axis=1)\n",
    "                normX1[normX1 <= 1e-15] = 1\n",
    "                normX2[normX2 <= 1e-15] = 1\n",
    "                X1 = X1 / np.sqrt(normX1[:, None])\n",
    "                X2 = X2 / np.sqrt(normX2[:, None])\n",
    "                R = orthogonal_procrustes(X1, X2)[0]\n",
    "                return np.linalg.norm(X1 @ R - X2)\n",
    "        else:\n",
    "            # in the omni case we don't need to align\n",
    "            return np.linalg.norm(X1 - X2)\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Fits the test to the two input graphs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A1, A2 : nx.Graph, nx.DiGraph, nx.MultiDiGraph, nx.MultiGraph, np.ndarray\n",
    "            The two graphs to run a hypothesis test on.\n",
    "            If np.ndarray, shape must be ``(n_vertices, n_vertices)`` for both graphs,\n",
    "            where ``n_vertices`` is the same for both\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        X_hats = (X1, X2)\n",
    "        sample_T_statistic = self._difference_norm(X_hats[0], X_hats[1])\n",
    "        null_distribution_1 = self._bootstrap(X_hats[0])\n",
    "        null_distribution_2 = self._bootstrap(X_hats[1])\n",
    "\n",
    "        # uisng exact mc p-values (see, for example, Phipson and Smyth, 2010)\n",
    "        p_value_1 = (\n",
    "            len(null_distribution_1[null_distribution_1 >= sample_T_statistic]) + 1\n",
    "        ) / (self.n_bootstraps + 1)\n",
    "        p_value_2 = (\n",
    "            len(null_distribution_2[null_distribution_2 >= sample_T_statistic]) + 1\n",
    "        ) / (self.n_bootstraps + 1)\n",
    "\n",
    "        p_value = max(p_value_1, p_value_2)\n",
    "\n",
    "        self.null_distribution_1_ = null_distribution_1\n",
    "        self.null_distribution_2_ = null_distribution_2\n",
    "        self.sample_T_statistic_ = sample_T_statistic\n",
    "        self.p_value_1_ = p_value_1\n",
    "        self.p_value_2_ = p_value_2\n",
    "        self.p_value_ = p_value\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define K-Neighrest neighboor function\n",
    "def KNN(X, k):\n",
    "    '''This function computes k nearest neighboors of rows of a matrix from the first row and outputs a tuple containing the rows of the knn. X must be a matrix'''\n",
    "    distance = []\n",
    "    for i in range(1,len(X)):\n",
    "        dist = LA.norm(X[i]-X[i-1], ord = 2)\n",
    "        distance.append(dist)\n",
    "    smallest_k = heapq.nsmallest(k,((m, n) for n, m in enumerate(distance)))\n",
    "    _, index_smallest =  zip(*smallest_k)\n",
    "    index_smallest = np.array(index_smallest)\n",
    "    return index_smallest\n",
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n"
     ]
    }
   ],
   "source": [
    "#Need to simulate MMSBM with B being a a dxd matrix with each entry being equal to q = 0.3 and diagonal being equal to p = 0.8. E is a dx1 vector with each entry being 0.3. Finally the Z matrix(n_vertices x d) is made up by Z(i) vector drawn from Dirichlet(theta* 1d) where theta is a parameter between 0 and 1 and 1d is a d-dimensional vector with d entries equal to 1. d = 4\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "q = 0.3\n",
    "p = 0.8\n",
    "d = 4\n",
    "theta = 1 #parameter to fine tune the MMSBM\n",
    "k = 100 #knearest neighboor \n",
    "\n",
    "n = 400 #number of vertices\n",
    "time_series = 12\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "B = np.full((d,d), q) - (np.array([q, q, q, q])) * np.identity(d)\n",
    "diag_B = (np.array([p, p, p, p])) * np.identity(d)\n",
    "\n",
    "B = B + diag_B\n",
    "\n",
    "B_sqrt = sqrtm(B)\n",
    "X_list = []\n",
    "\n",
    "E = 0.11*np.ones((1,d))\n",
    "G = []\n",
    "Z_list = [] #list conytaining each Z\n",
    "alpha = np.ones(d)*theta\n",
    "\n",
    "for j in range(time_series):\n",
    "    Z = []\n",
    "    for i in range(n):\n",
    "        z = np.random.dirichlet(alpha)\n",
    "        Z.append(z)\n",
    "\n",
    "    Z = np.array(Z)\n",
    "    Z_list.append(Z)\n",
    "    x = Z @ B_sqrt\n",
    "    X_list.append(x)\n",
    "    #g = np.random.binomial(1, Z @ B @ Z.T)\n",
    "    g = rdpg(x)\n",
    "    G.append(g)\n",
    "\n",
    "X = np.array(X_list)\n",
    "G = np.array(G)\n",
    "#Now add perturbation at time 6 and 7\n",
    "\n",
    "#To add perturbation find k=100 neighest neighboors of X(1) of X where X is the common latent position to all graphs besides perturbation and X(1) is the first row. For each row corresponding to one of the 100th neighrest neighboor the value of delta will be E, else it will be zero\n",
    "\n",
    "index_to_pert = np.sort(KNN(X[0], k))\n",
    "\n",
    "#Now that I have indexes to pertubate matrix, create delta by assigning a d-dimensional vector E to every row in the indexes found, else zero\n",
    "\n",
    "delta = np.zeros(np.shape(X[0]))\n",
    "\n",
    "for j in index_to_pert:\n",
    "    delta[j] = E\n",
    "\n",
    "#Now perturb time point 6\n",
    "\n",
    "X[5] = X[5] + theta * delta #may need to change theta to a constant\n",
    "\n",
    "#Now perturb time point 7\n",
    "\n",
    "X[6] = X[6] - theta * delta  #here it may fail beacuse pertubations adds 0.3 and some entries are 0.856\n",
    "\n",
    "print(X[6].shape)\n",
    "\n",
    "#Finally add perturbed graph back into the time series of graphs\n",
    "\n",
    "G[5] = rdpg(X[5])\n",
    "G[6] = rdpg(X[6])\n",
    "\n",
    "#print(((X[5] @ X[5].T)>1).sum())\n",
    "#G[5] = np.random.binomial(1, X[5] @ X[5].T)\n",
    "#G[6] = np.random.binomial(1, X[6] @ X[5].T)\n",
    "\n",
    "#Now finally generated all the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 400, 2)\n",
      "[[[ 9.73642491  0.28083816]\n",
      "  [10.70244511 -0.26336759]\n",
      "  [ 9.76597061  0.07133293]\n",
      "  ...\n",
      "  [10.17105778  0.05235695]\n",
      "  [10.49966303  0.41793056]\n",
      "  [ 8.85004468  0.15807448]]\n",
      "\n",
      " [[ 6.99719964 -0.23071342]\n",
      "  [ 7.08483413  0.1321752 ]\n",
      "  [ 6.79537553 -0.08955746]\n",
      "  ...\n",
      "  [ 7.05398491 -0.07848035]\n",
      "  [ 7.66773511 -0.32640238]\n",
      "  [ 6.25712676 -0.14416442]]]\n"
     ]
    }
   ],
   "source": [
    "#Embed according to algo 1\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def algo1(A, embedding, s=2):\n",
    "    t=0\n",
    "    Zhat_list = []\n",
    "    while(t+s-1 <= len(A)-1):\n",
    "        if(embedding == 'omnibus'):\n",
    "            embedder = OmnibusEmbed()\n",
    "            Zhat = embedder.fit_transform(A[t:s+t])\n",
    "            Zhat_list.append(Zhat)\n",
    "        elif(embedding == 'MASE'):\n",
    "            embedder = MultipleASE()\n",
    "            V = embedder.fit_transform(A[t:s+t])\n",
    "            R = embedder.scores_ #For omni can jusr return R scores and use those to compute p-value\n",
    "            X = []\n",
    "            for i in range(len(R)):\n",
    "                #R_sqrt = sqrtm(R[i])\n",
    "                #x = V @ R_sqrt\n",
    "                x = V @ R[i]\n",
    "                X.append(x)\n",
    "            X = np.array(X)\n",
    "            Zhat_list.append(X)\n",
    "                \n",
    "        t = t+1\n",
    "    return(Zhat_list)\n",
    "\n",
    "#Embed with s = 12\n",
    "\n",
    "MASE_embedding = algo1(G, embedding = 'MASE', s=2)\n",
    "OMNI_embedding = algo1(G, embedding = 'omnibus', s=2)\n",
    "\n",
    "\n",
    "MASE = MASE_embedding[5]\n",
    "OMNI = OMNI_embedding[5]\n",
    "\n",
    "print(MASE.shape)\n",
    "print(MASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now need to compute pvalues for time 6 and 7, in montecarlo fashion using number of simulations equal to 100\n",
    "montecarlo = 50\n",
    "sig_p = []\n",
    "\n",
    "lpt_X = LatentPositionTest_X(n_bootstraps=10)\n",
    "lpt_X = LatentPositionTest_X(n_bootstraps=100, n_components=d)\n",
    "\n",
    "for i in range(montecarlo):\n",
    "    lpt_X.fit(MASE[0],MASE[1])\n",
    "    sig_p.append(lpt_X.p_value_)\n",
    "    print('p = {}'.format(lpt_X.p_value_))\n",
    "\n",
    "power_MASE = ((sig_p<=0.05).sum())/montecarlo\n",
    "print(power_MASE)\n",
    "\n",
    "sig_p = []\n",
    "\n",
    "for i in range(montecarlo):\n",
    "    lpt_X.fit(OMNI[0],OMNI[1])\n",
    "    sig_p.append(lpt_X.p_value_)\n",
    "    print('p = {}'.format(lpt_X.p_value_))\n",
    "\n",
    "power_OMNI = ((sig_p<=0.05).sum())/montecarlo\n",
    "print(power_OMNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.009900990099009901\n"
     ]
    }
   ],
   "source": [
    "lpt = LatentPositionTest(n_bootstraps=100, n_components=d, embedding='omnibus')\n",
    "lpt.fit(G[5], G[6])\n",
    "print('p = {}'.format(lpt.p_value_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
